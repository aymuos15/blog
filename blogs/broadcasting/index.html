<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding Broadcasting - Machine Learning, Through PyTorch Code</title>
    <link rel="stylesheet" href="../../styles.css">
    <link rel="stylesheet" href="broadcasting.css">
    <script src="broadcasting.js" defer></script>
</head>
<body>
    <nav>
        <a href="../../index.html">← Home</a>
    </nav>

    <h1>Understanding Broadcasting</h1>
    <p class="subtitle">and how PyTorch automatically expands tensors for element-wise operations.</p>

    <h2>What Is Broadcasting?</h2>
    <p>Broadcasting allows PyTorch to perform operations on tensors of different shapes without explicitly copying data. It automatically <strong>expands</strong> smaller tensors to match larger ones.</p>

    <div class="highlight-box">
        <strong>Key Concept:</strong> Broadcasting makes your code cleaner and more memory-efficient by avoiding explicit loops and tensor copying.
    </div>

    <h2>Broadcasting Rules</h2>
    <p>PyTorch follows these rules to determine if tensors can be broadcast together:</p>

    <div class="rules-box">
        <ol>
            <li>Start with the <strong>trailing dimensions</strong> and work forward</li>
            <li>Two dimensions are compatible if:
                <ul>
                    <li>They are equal, OR</li>
                    <li>One of them is 1</li>
                </ul>
            </li>
            <li>If one tensor has fewer dimensions, prepend 1s to its shape</li>
        </ol>
    </div>

    <h2>Visual Example</h2>
    <p>See how a (3,1) tensor broadcasts with a (1,4) tensor:</p>

    <div class="visual-demo">
        <h3 style="text-align: center; margin-bottom: 20px;">Broadcasting in Action</h3>
        <p style="text-align: center; color: #666; margin-bottom: 30px;">Watch how tensors expand to match shapes</p>

        <div class="broadcast-container">
            <div class="tensor-section">
                <div class="tensor-label">Tensor A: (3, 1)</div>
                <div class="tensor-display" id="tensorA"></div>
            </div>

            <div class="operator">+</div>

            <div class="tensor-section">
                <div class="tensor-label">Tensor B: (1, 4)</div>
                <div class="tensor-display" id="tensorB"></div>
            </div>

            <div class="arrow-down-large">↓</div>

            <div class="broadcast-section">
                <div class="tensor-label">After Broadcasting</div>
                <div class="broadcast-display">
                    <div class="expanded-tensor">
                        <div class="tensor-sublabel">A expanded: (3, 4)</div>
                        <div class="tensor-display" id="tensorAExpanded"></div>
                    </div>
                    <div class="operator-small">+</div>
                    <div class="expanded-tensor">
                        <div class="tensor-sublabel">B expanded: (3, 4)</div>
                        <div class="tensor-display" id="tensorBExpanded"></div>
                    </div>
                </div>
            </div>

            <div class="arrow-down-large">↓</div>

            <div class="tensor-section">
                <div class="tensor-label">Result: (3, 4)</div>
                <div class="tensor-display" id="tensorResult"></div>
            </div>
        </div>

        <div class="control-buttons">
            <button onclick="animateBroadcast()">Show Broadcasting</button>
            <button onclick="resetBroadcast()">Reset</button>
        </div>
    </div>

    <h2>Broadcasting Examples</h2>
    <p>Here are common broadcasting patterns in PyTorch:</p>

    <div class="code-block"><span class="keyword">import</span> torch

<span class="comment"># Example 1: Add scalar to tensor</span>
x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],
                  [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])  <span class="comment"># Shape: (2, 3)</span>
y = x + <span class="number">10</span>                           <span class="comment"># Scalar broadcasts to (2, 3)</span>
<span class="comment"># Result: [[11, 12, 13], [14, 15, 16]]</span>

<span class="comment"># Example 2: Add row vector to matrix</span>
x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],
                  [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])  <span class="comment"># Shape: (2, 3)</span>
row = torch.tensor([<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>])  <span class="comment"># Shape: (3,) → broadcasts to (1, 3) → (2, 3)</span>
y = x + row
<span class="comment"># Result: [[11, 22, 33], [14, 25, 36]]</span>

<span class="comment"># Example 3: Add column vector to matrix</span>
col = torch.tensor([[<span class="number">10</span>], [<span class="number">20</span>]])    <span class="comment"># Shape: (2, 1) → broadcasts to (2, 3)</span>
y = x + col
<span class="comment"># Result: [[11, 12, 13], [24, 25, 26]]</span>

<span class="comment"># Example 4: Matrix + matrix with broadcasting</span>
a = torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]])  <span class="comment"># Shape: (3, 1)</span>
b = torch.tensor([[<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>]])  <span class="comment"># Shape: (1, 3)</span>
y = a + b                       <span class="comment"># Both broadcast to (3, 3)</span>
<span class="comment"># Result: [[11, 21, 31],
#          [12, 22, 32],
#          [13, 23, 33]]</span></div>

    <h2>When Broadcasting Fails</h2>
    <p>Not all tensor shapes are compatible for broadcasting:</p>

    <div class="code-block"><span class="comment"># This will fail!</span>
a = torch.randn(<span class="number">3</span>, <span class="number">4</span>)  <span class="comment"># Shape: (3, 4)</span>
b = torch.randn(<span class="number">2</span>, <span class="number">3</span>)  <span class="comment"># Shape: (2, 3)</span>
c = a + b              <span class="comment"># RuntimeError: shapes cannot be broadcast</span>

<span class="comment"># Why? Compare dimensions from right to left:</span>
<span class="comment"># a: (3, 4)</span>
<span class="comment"># b: (2, 3)</span>
<span class="comment">#     ^  ^</span>
<span class="comment">#     |  └─ 4 ≠ 3 and neither is 1 ✗</span>
<span class="comment">#     └──── 3 ≠ 2 and neither is 1 ✗</span></div>

    <div class="highlight-box">
        <strong>Common Use Case:</strong> Normalizing batches<br><br>
        <code>batch = (batch - mean) / std</code><br><br>
        If batch is (N, C, H, W) and mean/std are (C, 1, 1), broadcasting handles the normalization across all N samples.
    </div>

    <h2>Summary</h2>
    <div class="highlight-box">
        <strong>Broadcasting</strong> automatically expands tensors to compatible shapes.<br><br>
        Dimensions must be <strong>equal or 1</strong> when compared from right to left.<br><br>
        Broadcasting is <strong>memory-efficient</strong> — no data is actually copied, just virtually repeated.
    </div>

    <footer>
        A minimal explanation of neural network backpropagation
    </footer>
</body>
</html>
