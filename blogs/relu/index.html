<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding ReLU - Machine Learning, Through PyTorch Code</title>
    <link rel="stylesheet" href="../../styles.css">
    <link rel="stylesheet" href="relu.css">
    <script src="relu.js" defer></script>
</head>
<body>
    <nav>
        <a href="../../index.html">← Home</a>
    </nav>

    <h1>Understanding ReLU</h1>
    <p class="subtitle">and the difference between the Forward and Backward Passes for a diffrentiable function.</p>

    <h2>What is ReLU?</h2>
    <p>ReLU (Rectified Linear Unit) is a simple activation function used in neural networks. If the input is <strong>positive</strong>, keep it. If <strong>negative or zero</strong>, output 0.</p>
    <div class="highlight-box">
        <strong>Mathematical definition:</strong> ReLU(x) = max(0, x)
    </div>

<div class="visual-demo">
    <h3 style="text-align: center; margin-bottom: 20px;">Interactive ReLU Graph</h3>
    <p style="text-align: center; color: #666; margin-bottom: 30px;">Move the slider to see how different input values are transformed by ReLU</p>
    <canvas id="reluGraphDynamic" width="700" height="300" style="width: 100%; height: auto; border: 1px solid #000;"></canvas>
    <div style="margin-top: 20px;">
        <label style="display: block; text-align: center; margin-bottom: 10px;">
            Input value (x): <strong id="xValue">0</strong>
        </label>
        <input type="range" id="xSlider" min="-3" max="3" step="0.1" value="0"
               style="width: 100%; cursor: pointer;">
        <div style="display: flex; justify-content: space-between; margin-top: 10px; font-size: 0.9em; color: #666;">
            <span>-3</span>
            <span>Output: <strong id="yValue">0</strong></span>
            <span>3</span>
        </div>
    </div>
</div>

    <h2>The Forward Pass</h2>
    <p>The forward pass computes the <strong>actual ReLU function</strong>:</p>
    <div class="code-tabs">
        <div class="tab-buttons">
            <button class="tab-button active" onclick="switchTab(event, 'forward-simple')">Simple</button>
            <button class="tab-button" onclick="switchTab(event, 'forward-optimized')">Optimized</button>
        </div>
        <div id="forward-simple" class="tab-content active">
            <div class="code-block"><span class="keyword">def</span> <span class="function">forward</span>(ctx, x):
    <span class="comment"># Save input for backward pass</span>
    ctx.save_for_backward(x)

    <span class="comment"># Return max of x and zeros</span>
    <span class="keyword">return</span> torch.max(x, torch.zeros_like(x))</div>
        </div>
        <div id="forward-optimized" class="tab-content">
            <div class="code-block"><span class="keyword">def</span> <span class="function">forward</span>(ctx, x):
    <span class="comment"># Save input for backward pass</span>
    ctx.save_for_backward(x)

    <span class="comment"># More efficient: clamp avoids creating zero tensor</span>
    <span class="keyword">return</span> x.clamp(min=<span class="number">0</span>)</div>
        </div>
    </div>

    <div class="visual-explanation">
        <div class="column">
            <h3>Input</h3>
            <div>[-2, 0, 3]</div>
        </div>
        <div class="arrow">→</div>
        <div class="column">
            <h3>Apply ReLU</h3>
            <div>max(0, x)</div>
        </div>
        <div class="arrow">→</div>
        <div class="column">
            <h3>Output</h3>
            <div>[0, 0, 3]</div>
        </div>
    </div>

    <div class="visual-demo">
        <h3 style="text-align: center; margin-bottom: 20px;">Forward Pass Visualization</h3>
        <p style="text-align: center; color: #666; margin-bottom: 30px;">Watch how ReLU transforms each element</p>
        <div class="matrix-container" id="forwardViz">
            <div class="matrix">
                <div class="matrix-label">Input</div>
                <div class="matrix-grid" id="forwardInput" style="grid-template-columns: repeat(3, 1fr);"></div>
            </div>
            <div class="operation">→</div>
            <div class="matrix">
                <div class="matrix-label">ReLU</div>
                <div class="matrix-grid" id="forwardOperation" style="grid-template-columns: repeat(3, 1fr);"></div>
            </div>
            <div class="operation">→</div>
            <div class="matrix">
                <div class="matrix-label">Output</div>
                <div class="matrix-grid" id="forwardOutput" style="grid-template-columns: repeat(3, 1fr);"></div>
            </div>
        </div>
        <div class="control-buttons">
            <button onclick="animateForward()">Animate Forward Pass</button>
            <button onclick="resetForward()">Reset</button>
        </div>
    </div>

    <h2>The Backward Pass</h2>
    <p>The backward pass computes the <strong>derivative (gradient)</strong> of ReLU:</p>
    <div class="code-tabs">
        <div class="tab-buttons">
            <button class="tab-button active" onclick="switchTab(event, 'backward-simple')">Simple</button>
            <button class="tab-button" onclick="switchTab(event, 'backward-optimized')">Optimized</button>
        </div>
        <div id="backward-simple" class="tab-content active">
            <div class="code-block"><span class="keyword">def</span> <span class="function">backward</span>(ctx, grad_output):
    <span class="comment"># Retrieve saved input</span>
    input_x, = ctx.saved_tensors

    <span class="comment"># Clone gradient to avoid modifying original</span>
    grad_input = grad_output.clone()

    <span class="comment"># Set gradient to 1 where x > 0</span>
    grad_input[input_x > <span class="number">0</span>] *= <span class="number">1</span>

    <span class="comment"># Set gradient to 0 where x <= 0</span>
    grad_input[input_x <= <span class="number">0</span>] *= <span class="number">0</span>

    <span class="keyword">return</span> grad_input</div>
        </div>
        <div id="backward-optimized" class="tab-content">
            <div class="code-block"><span class="keyword">def</span> <span class="function">backward</span>(ctx, grad_output):
    <span class="comment"># Retrieve saved input</span>
    input_x, = ctx.saved_tensors

    <span class="comment"># More efficient: directly multiply by boolean mask</span>
    <span class="keyword">return</span> grad_output * (input_x > <span class="number">0</span>)</div>
        </div>
    </div>

    <div class="highlight-box">
        <strong>The derivative tells us how much the output changes when we change the input.</strong><br><br>
        Derivative of ReLU:<br>
        If x > 0: derivative = <strong>1</strong> (slope is 1)<br>
        If x ≤ 0: derivative = <strong>0</strong> (flat, no change)
    </div>

    <div class="visual-explanation">
        <div class="column">
            <h3>Gradient In</h3>
            <div>[1, 1, 1]</div>
        </div>
        <div class="arrow">→</div>
        <div class="column">
            <h3>Derivative Mask</h3>
            <div>(x > 0)<br>[0, 0, 1]</div>
        </div>
        <div class="arrow">→</div>
        <div class="column">
            <h3>Gradient Out</h3>
            <div>[0, 0, 1]</div>
        </div>
    </div>

    <div class="visual-demo">
        <h3 style="text-align: center; margin-bottom: 20px;">Backward Pass Visualization</h3>
        <p style="text-align: center; color: #666; margin-bottom: 30px;">Watch how gradients flow through the derivative mask</p>
        <div class="matrix-container" id="backwardViz">
            <div class="matrix">
                <div class="matrix-label">Gradient In</div>
                <div class="matrix-grid" id="backwardGradIn" style="grid-template-columns: repeat(3, 1fr);"></div>
            </div>
            <div class="operation">×</div>
            <div class="matrix">
                <div class="matrix-label">Derivative Mask</div>
                <div class="matrix-grid" id="backwardMask" style="grid-template-columns: repeat(3, 1fr);"></div>
            </div>
            <div class="operation">=</div>
            <div class="matrix">
                <div class="matrix-label">Gradient Out</div>
                <div class="matrix-grid" id="backwardGradOut" style="grid-template-columns: repeat(3, 1fr);"></div>
            </div>
        </div>
        <div class="control-buttons">
            <button onclick="animateBackward()">Animate Backward Pass</button>
            <button onclick="resetBackward()">Reset</button>
        </div>
    </div>

    <h2>Why They're Different</h2>
    <p><strong>Forward</strong> transforms the <strong>data</strong> by applying the ReLU function.</p>
    <p><strong>Backward</strong> transforms the <strong>gradients</strong> by applying the ReLU derivative.</p>

    <div class="highlight-box">
        <strong>Example with x = -2:</strong><br>
        Forward: ReLU(-2) = 0 (the function value)<br>
        Backward: derivative = 0 (because the function is flat when x < 0)
    </div>

    <h2>Interactive Demo</h2>
    <div class="interactive-demo">
        <p style="text-align: center; margin-bottom: 30px;">Enter values to see the forward and backward passes in action</p>

        <div class="input-group">
            <label>Input Values (x)</label>
            <input type="text" id="inputValues" value="-2, 0, 3">
        </div>
        <div class="input-group">
            <label>Gradient from Above (∂L/∂y)</label>
            <input type="text" id="gradValues" value="1, 1, 1">
        </div>
        <button onclick="compute()">Compute</button>

        <div id="results"></div>
    </div>

    <h2>Summary</h2>
    <div class="highlight-box">
        Forward and backward passes use different code because they compute <strong>different mathematical operations</strong>.<br><br>
        <strong>Forward</strong> computes the function ReLU(x).<br>
        <strong>Backward</strong> computes the derivative of ReLU.<br><br>
        They are related but <strong>mathematically distinct</strong>.
    </div>

    <footer>
        A minimal explanation of neural network backpropagation
    </footer>
</body>
</html>
